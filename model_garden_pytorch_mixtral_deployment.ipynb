{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Vertex AI Model Garden - Mixtral 8x7B Model (Deployment)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi-aGs442yve"
      },
      "source": [
        "## Run the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7-wMw81Ty4Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1715928125202,
          "user_tz": -330,
          "elapsed": 21317,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "e7cd4caf-60d5-408d-d4bf-2a834eb5e36b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enabling Vertex AI API and Compute Engine API.\n",
            "Operation \"operations/acat.p2-49759099991-1ca29af1-d594-4c5e-826f-f920ab48c6a9\" finished successfully.\n",
            "Initializing Vertex AI API.\n",
            "Using this default Service Account: 49759099991-compute@developer.gserviceaccount.com\n",
            "Using this GCS Bucket: gs://llm-hosting-mixtral\n",
            "No changes made to gs://llm-hosting-mixtral/\n",
            "Updated property [core/project].\n"
          ]
        }
      ],
      "source": [
        "# @title Setup Google Cloud project\n",
        "\n",
        "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "# @markdown 2. [Optional] [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs. Set the BUCKET_URI for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the same region as where the notebook was launched. Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\"). If not set, a unique GCS bucket will be created instead.\n",
        "\n",
        "# Import the necessary packages\n",
        "import os\n",
        "from datetime import datetime\n",
        "from typing import Tuple\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "# Get the default cloud project id.\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "\n",
        "# Get the default region for launching jobs.\n",
        "REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
        "\n",
        "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
        "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
        "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
        "\n",
        "# Cloud Storage bucket for storing the experiment artifacts.\n",
        "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
        "# prefer using your own GCS bucket, change the value yourself below.\n",
        "now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "BUCKET_URI = \"gs://llm-hosting-mixtral\"  # @param {type:\"string\"}\n",
        "assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
        "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}\"\n",
        "\n",
        "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
        "\n",
        "# Initialize Vertex AI API.\n",
        "print(\"Initializing Vertex AI API.\")\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "# Gets the default BUCKET_URI and SERVICE_ACCOUNT if they were not specified by the user.\n",
        "SERVICE_ACCOUNT = None\n",
        "shell_output = ! gcloud projects describe $PROJECT_ID\n",
        "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
        "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
        "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
        "\n",
        "# Create a unique GCS bucket for this notebook, if not specified by the user.\n",
        "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
        "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}\"\n",
        "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
        "else:\n",
        "    shell_output = ! gsutil ls -Lb {BUCKET_URI} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
        "    bucket_region = shell_output[0].strip().lower()\n",
        "    if bucket_region != REGION:\n",
        "        raise ValueError(\n",
        "            \"Bucket region %s is different from notebook region %s\"\n",
        "            % (bucket_region, REGION)\n",
        "        )\n",
        "\n",
        "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
        "\n",
        "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
        "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
        "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
        "\n",
        "! gcloud config set project $PROJECT_ID\n",
        "\n",
        "# The pre-built serving docker images with vLLM\n",
        "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240313_0916_RC00\"\n",
        "\n",
        "\n",
        "def get_job_name_with_datetime(prefix: str) -> str:\n",
        "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
        "    jobs in Vertex AI.\n",
        "    \"\"\"\n",
        "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
        "\n",
        "\n",
        "def deploy_model_vllm(\n",
        "    model_name: str,\n",
        "    model_id: str,\n",
        "    service_account: str,\n",
        "    machine_type: str = \"n1-highmem-96\",\n",
        "    accelerator_type: str = \"NVIDIA_TESLA_V100\",\n",
        "    accelerator_count: int = 1,\n",
        "    max_model_len: int = 4096,\n",
        "    gpu_memory_utilization=0.9,\n",
        "    use_openai_server: bool = False,\n",
        "    use_chat_completions_if_openai_server: bool = False,\n",
        ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
        "    \"\"\"Deploys Mistral models with vLLM on Vertex AI.\n",
        "\n",
        "    Args:\n",
        "        model_name: Display name of the model.\n",
        "        model_id: Model ID or path to model weights.\n",
        "        service_account: Service account for model uploading and deployment.\n",
        "        machine_type: Deployment machine type.\n",
        "        accelerator_type: Deployment accelerator type.\n",
        "        accelerator_count: Number of accelerators to use.\n",
        "        max_model_len: Maximum model length.\n",
        "        gpu_memory_utilization: Fraction of GPU memory to be used for the model\n",
        "            executor.\n",
        "        use_openai_server: Whether to use the OpenAI-format vLLM model server.\n",
        "        use_chat_completions_if_openai_server: If the OpenAI model server is\n",
        "            used, whether to use the chat completion API as opposed to the text\n",
        "            completion API. The vLLM text completion API mimics the OpenAI text\n",
        "            completion API:\n",
        "            https://platform.openai.com/docs/api-reference/completions/create.\n",
        "            It has two required parameters: the model ID to direct requests to\n",
        "            and the prompt. The response includes a \"choices\" field that\n",
        "            contains the generated text and a \"usage\" field that contains token\n",
        "            counts. The vLLM chat completion API mimics the OpenAI chat\n",
        "            completion API:\n",
        "            https://platform.openai.com/docs/api-reference/chat/create. It has\n",
        "            two required parameters: the model ID to direct requests to and\n",
        "            \"messages\" which is a sequence of system/user/assistant/tool\n",
        "            messages that can represent a multi-turn chat conversation. The\n",
        "            response includes a \"choices\" field that contains the generated\n",
        "            message from a role and a \"usage\" field that contains token counts.\n",
        "\n",
        "    Returns:\n",
        "        Model instance and endpoint instance.\n",
        "    \"\"\"\n",
        "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
        "\n",
        "    dtype = \"bfloat16\"\n",
        "    if accelerator_type in [\"NVIDIA_TESLA_T4\", \"NVIDIA_TESLA_V100\"]:\n",
        "        dtype = \"float16\"\n",
        "\n",
        "    if \"asia\" in REGION:\n",
        "        region_suffix = \"asia\"\n",
        "    elif \"europe\" in REGION:\n",
        "        region_suffix = \"eu\"\n",
        "    else:\n",
        "        region_suffix = \"us\"\n",
        "\n",
        "    vllm_args = [\n",
        "        \"--host=0.0.0.0\",\n",
        "        \"--port=7080\",\n",
        "        f\"--model=gs://vertex-model-garden-public-{region_suffix}/{model_id}\",\n",
        "        f\"--tensor-parallel-size={accelerator_count}\",\n",
        "        \"--swap-space=16\",\n",
        "        f\"--dtype={dtype}\",\n",
        "        f\"--gpu-memory-utilization={gpu_memory_utilization}\",\n",
        "        f\"--max-model-len={max_model_len}\",\n",
        "        \"--disable-log-stats\",\n",
        "    ]\n",
        "    serving_env = {\n",
        "        \"MODEL_ID\": model_id,\n",
        "        \"DEPLOY_SOURCE\": \"notebook\",\n",
        "    }\n",
        "    if use_openai_server:\n",
        "        if use_chat_completions_if_openai_server:\n",
        "            serving_container_predict_route = \"/v1/chat/completions\"\n",
        "        else:\n",
        "            serving_container_predict_route = \"/v1/completions\"\n",
        "    else:\n",
        "        serving_container_predict_route = \"/generate\"\n",
        "    model = aiplatform.Model.upload(\n",
        "        display_name=model_name,\n",
        "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
        "        serving_container_command=[\n",
        "            \"python\",\n",
        "            \"-m\",\n",
        "            (\n",
        "                \"vllm.entrypoints.api_server\"\n",
        "                if not use_openai_server\n",
        "                else \"vllm.entrypoints.openai.api_server\"\n",
        "            ),\n",
        "        ],\n",
        "        serving_container_args=vllm_args,\n",
        "        serving_container_ports=[7080],\n",
        "        serving_container_predict_route=serving_container_predict_route,\n",
        "        serving_container_health_route=\"/health\",\n",
        "        serving_container_environment_variables=serving_env,\n",
        "    )\n",
        "\n",
        "    model.deploy(\n",
        "        endpoint=endpoint,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_type=accelerator_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        deploy_request_timeout=1800,\n",
        "        service_account=service_account,\n",
        "    )\n",
        "    return model, endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PONphEqp2rE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53e95bfe-f638-4574-eeff-d3a55bdc4668",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1715931332354,
          "user_tz": -330,
          "elapsed": 2068218,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
            "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/49759099991/locations/us-central1/endpoints/829038364412870656/operations/2089265091424813056\n",
            "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/49759099991/locations/us-central1/endpoints/829038364412870656\n",
            "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
            "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/49759099991/locations/us-central1/endpoints/829038364412870656')\n",
            "INFO:google.cloud.aiplatform.models:Creating Model\n",
            "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/49759099991/locations/us-central1/models/8169724337608196096/operations/2120790288816406528\n",
            "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/49759099991/locations/us-central1/models/8169724337608196096@1\n",
            "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
            "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/49759099991/locations/us-central1/models/8169724337608196096@1')\n",
            "INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/49759099991/locations/us-central1/endpoints/829038364412870656\n",
            "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/49759099991/locations/us-central1/endpoints/829038364412870656/operations/8326750575332950016\n",
            "INFO:google.cloud.aiplatform.models:Endpoint model deployed. Resource name: projects/49759099991/locations/us-central1/endpoints/829038364412870656\n"
          ]
        }
      ],
      "source": [
        "# @title Deploy\n",
        "\n",
        "# @markdown This section uploads prebuilt Mixtral 8x7B model to Model Registry and deploys with [vLLM](https://github.com/vllm-project/vllm) to a Vertex AI Endpoint. It takes 15 minutes to 1 hour to finish depending on the model and the accelerator.\n",
        "\n",
        "# @markdown Set the model to deploy.\n",
        "\n",
        "model_id = \"mistralai/Mixtral-8x7B-v0.1\"  # @param [\"mistralai/Mixtral-8x7B-v0.1\", \"mistralai/Mixtral-8x7B-Instruct-v0.1\"]\n",
        "\n",
        "# Find Vertex AI prediction supported accelerators and regions in\n",
        "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute.\n",
        "\n",
        "# @markdown L4 GPUs are good serving solutions and are cost effective than A100s.\n",
        "\n",
        "accelerator_type = \"NVIDIA_L4\"  # @param [\"NVIDIA_L4\", \"NVIDIA_TESLA_A100\" , \"NVIDIA_TESLA_V100\"]\n",
        "\n",
        "if accelerator_type == \"NVIDIA_L4\":\n",
        "    machine_type = \"g2-standard-96\"\n",
        "    accelerator_count = 8\n",
        "elif accelerator_type == \"NVIDIA_TESLA_A100\":\n",
        "    machine_type = \"a2-highgpu-4g\"\n",
        "    accelerator_count = 4\n",
        "elif accelerator_type == \"NVIDIA_TESLA_V100\":\n",
        "    machine_type = \"n1-highmem-96\"\n",
        "    accelerator_count = 8\n",
        "\n",
        "# Larger setting of `max-model-len` can lead to higher requirements on\n",
        "# `gpu-memory-utilization` and GPU configuration. Larger setting of\n",
        "# `gpu-memory-utilization` increases the risk of running out of GPU memory with\n",
        "# long prompts.\n",
        "max_model_len = 4096\n",
        "gpu_memory_utilization = 0.85\n",
        "\n",
        "model, endpoint = deploy_model_vllm(\n",
        "    model_name=get_job_name_with_datetime(prefix=\"mixtral-serve-vllm\"),\n",
        "    model_id=model_id,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    max_model_len=max_model_len,\n",
        "    gpu_memory_utilization=gpu_memory_utilization,\n",
        "    use_openai_server=False,\n",
        "    use_chat_completions_if_openai_server=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYN1Z49SJ-MM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1715934006639,
          "user_tz": -330,
          "elapsed": 72161,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "69e7b7ec-84d3-4b03-a0c9-2de0d8b398cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt:\n",
            "What is a car?\n",
            "Output:\n",
            "A car is a four-wheeled vehicle that is driven by an internal-combustion engine. It has been around since the early 19^{th} century, but it only became popular after the Second World War. The car was invented by Carl Benz, and the first car was built in Germany in 1889. There are many different types of cars, but they all have one thing in common: they are all powered by engines.\n",
            "\n",
            "There are two main types of cars: petrol/gasoline cars and diesel cars. Petrol/gasoline cars use gasoline as their fuel source, while diesel cars use diesel fuel. Diesel engines are more powerful than gasoline engines, but they also require more maintenance. The main difference between gasoline and diesel engines is that diesel engines are more fuel-efficient, and they produce less emissions and pollution. They also last longer.\n",
            "\n",
            "What is a truck?\n",
            "\n",
            "A truck is a heavy vehicle designed to carry cargo. Trucks come in many sizes, from small pickup trucks to large semi-trailers. Trucks are used to haul goods, such as food, furniture, and machinery, and to transport people and cargo. Trucks are often used in the transportation industry, but they can also be used in other industries, including construction and agriculture. Trucks are generally powered by a diesel engine.\n",
            "\n",
            "Why choose a truck or an suv over a car?\n",
            "\n",
            "Choosing a truck or an SUV over a car has many advantages. First, trucks and SUVs are more spacious than cars. They have more legroom and headroom, and they have better cargo space. Second, trucks and SUVs are safer than cars. The extra weight and size makes them easier to stop and manoeuvre, and they are less likely to roll over. Third, trucks and SUVs are more comfortable than cars. They have better suspension, and they are usually quieter.\n",
            "\n",
            "How to choose one of these?\n",
            "\n",
            "There are many reasons why someone might choose an SUV or a truck over a car. For example, someone might need a vehicle with a lot of cargo space or someone might need a vehicle that is capable of towing heavy loads. Another reason is that some people prefer the way SUVs and trucks look and feel compared to cars. They may also prefer the way these vehicles handle on the road.\n",
            "\n",
            "What is a truck?\n",
            "\n",
            "A truck is a large, heavy vehicle designed mainly for transporting goods. Trucks come in many different sizes, from small pickup trucks to massive semi-trailers. Trucks typically have four wheels, though some models have more. They are often powered by large diesel engines.\n",
            "\n",
            "What is a sports utility vehicle (SUV)?\n",
            "\n",
            "An SUV is a larger version of a car. These vehicles have many of the same features as a car, but they also offer extra capabilities, such as towing and off-road performance. SUVs are usually equipped with four-wheel drive, which means that all four wheels can move at the same time. This is useful if the vehicle gets stuck in mud or snow.\n",
            "\n",
            "What is a sedan?\n",
            "\n",
            "A sedan is a smaller version of a car. They are usually equipped with two doors, though there are some four-door models available. Sedans are generally more fuel efficient than trucks and SUVs, but they do not have as much power.\n",
            "\n",
            "What is a coupe? A coupe is a smaller version of a car. They are usually equipped with two doors, but there are some three and four-door models available. Coupes are generally more fuel efficient than trucks and SUVs, and they usually have more power.\n",
            "\n",
            "What is a convertible?\n",
            "\n",
            "A convertible is a type of car that has a retractable roof. Most convertibles have a hard top that folds away into the trunk. This makes it possible to drive with the top down during nice weather conditions.\n",
            "\n",
            "What is the difference between a truck and an SUV?\n",
            "\n",
            "The most obvious difference between a truck and an SUV is the way they look. Trucks usually have a boxy shape, while SUVs tend to have more rounded edges. They are also generally larger than cars, with most having seating for at least four people.\n",
            "\n",
            "What is the difference between a sedan and a coupe?\n",
            "\n",
            "A sedan is generally smaller than a coupe. They usually have two doors, although some four-door models exist. Sedans are typically more fuel efficient than coupes, and they are easier to drive.\n",
            "\n",
            "What is the difference between a convertible and a coupe?\n",
            "\n",
            "A convertible is similar to a coupe, except that the roof can be removed. This makes it possible to drive with the top down during nice weather conditions.\n",
            "\n",
            "What is the difference between an SUV and a sedan?\n",
            "\n",
            "An SUV is usually bigger than a sedan, and it has more cargo space. They are also generally more comfortable to drive, and they are better at handling rough terrain.\n",
            "\n",
            "What is the difference between a sports car and a sedan?\n",
            "\n",
            "A sports car is usually smaller than a sedan, and it is designed to be driven fast. They are also generally more comfortable to drive than sedans, and they are much more fun to drive.\n",
            "\n",
            "What is the difference between a sports sedan and a sports car?\n",
            "\n",
            "A sports sedan is a smaller version of a sports car. These vehicles are designed to be driven fast, and they are also generally more comfortable to drive.\n",
            "\n",
            "What is the difference between a sports sedan and a convertible?\n",
            "\n",
            "A sports sedan is similar to a sports car, except that the roof can be removed. This makes it possible to drive with the top down during nice weather conditions.\n",
            "\n",
            "What is the difference between a coupe and a convertible?\n",
            "\n",
            "A coupe is similar to a convertible, except that it has a hard top that folds up into the trunk.\n"
          ]
        }
      ],
      "source": [
        "# @title Predict\n",
        "\n",
        "# @markdown Once deployment succeeds, you can send requests to the endpoint with text prompts. Sampling parameters supported by vLLM can be found [here](https://github.com/vllm-project/vllm/blob/2e8e49fce3775e7704d413b2f02da6d7c99525c9/vllm/sampling_params.py#L23-L64).\n",
        "\n",
        "# Loads an existing endpoint instance using the endpoint name:\n",
        "# - Using `endpoint_name = endpoint.name` allows us to get the endpoint name of\n",
        "#   the endpoint `endpoint` created in the cell above.\n",
        "# - Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load\n",
        "#   an existing endpoint with the ID 1234567890123456789.\n",
        "# You may uncomment the code below to load an existing endpoint.\n",
        "\n",
        "# endpoint_name = endpoint.name\n",
        "# # endpoint_name = \"\"  # @param {type:\"string\"}\n",
        "# aip_endpoint_name = (\n",
        "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
        "# )\n",
        "# endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
        "\n",
        "prompt = \"What is a car?\"  # @param {type: \"string\"}\n",
        "max_tokens = 8000  # @param {type:\"integer\"}\n",
        "temperature = 1.0  # @param {type:\"number\"}\n",
        "top_p = 1.0  # @param {type:\"number\"}\n",
        "top_k = 10  # @param {type:\"integer\"}\n",
        "\n",
        "instances = [\n",
        "    {\n",
        "        \"prompt\": prompt,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": top_p,\n",
        "        \"top_k\": top_k,\n",
        "    },\n",
        "]\n",
        "response = endpoint.predict(instances=instances)\n",
        "\n",
        "for prediction in response.predictions:\n",
        "    print(prediction)\n",
        "\n",
        "# Reference the following code for using the OpenAI vLLM server.\n",
        "# import json\n",
        "# response = endpoint.raw_predict(\n",
        "#     body=json.dumps({\n",
        "#         \"model\": prebuilt_model_id,\n",
        "#         \"prompt\": \"My favourite condiment is\",\n",
        "#         \"n\": 1,\n",
        "#         \"max_tokens\": 200,\n",
        "#         \"temperature\": 1.0,\n",
        "#         \"top_p\": 1.0,\n",
        "#         \"top_k\": 10,\n",
        "#     }),\n",
        "#     headers={\"Content-Type\": \"application/json\"},\n",
        "# )\n",
        "# print(response.json())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_garden_pytorch_mixtral_deployment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}