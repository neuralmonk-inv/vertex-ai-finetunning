{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tunning Mixtral 8x7B\n",
        "\n",
        "* accelerate is a distributed training library for PyTorch by HuggingFace. It allows you to train your models on multiple GPUs or CPUs in parallel (distributed configurations), which can significantly speed up training in presence of multiple GPUs (we won't use it in our example).\n",
        "* peft is a Python library by HuggingFace for efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters. PEFT methods only fine-tune a small number of (extra) model parameters, thereby greatly decreasing the computational and storage costs.\n",
        "* bitsandbytes by Tim Dettmers, is a lightweight wrapper around CUDA custom functions, in particular 8-bit optimizers, matrix multiplication (LLM.int8()), and quantization functions. It allows to run models stored in 4-bit precision: while 4-bit bitsandbytes stores weights in 4-bits, the computation still happens in 16 or 32-bit and here any combination can be chosen (float16, bfloat16, float32, and so on).\n",
        "* transformers is a Python library for natural language processing (NLP). It provides a number of pre-trained models for NLP tasks such as text classification, question answering, and machine translation.\n",
        "* trl is a full stack library by HuggingFace providing a set of tools to train transformer language models with Reinforcement Learning, from the Supervised Fine-tuning step (SFT), Reward Modeling step (RM) to the Proximal Policy Optimization (PPO) step."
      ],
      "metadata": {
        "id": "bgb1NQaoCacy"
      },
      "id": "bgb1NQaoCacy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installations and imports"
      ],
      "metadata": {
        "id": "dlDdLRywC7W_"
      },
      "id": "dlDdLRywC7W_"
    },
    {
      "cell_type": "code",
      "id": "aJ0w9BbxJOmWFlcJYKfsRNxO",
      "metadata": {
        "tags": [],
        "id": "aJ0w9BbxJOmWFlcJYKfsRNxO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e3ab152-678b-4cbf-f383-669c4fbc2faf",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1715673901394,
          "user_tz": -330,
          "elapsed": 171109,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "source": [
        "#!pip install -q -U torch --index-url https://download.pytorch.org/whl/cu117\n",
        "!pip install -q -U \"torch==2.1.2\"\n",
        "!pip install -q -U transformers==\"4.40.0\"\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U datasets\n",
        "!pip install -q -U trl\n",
        "!pip install -q -U peft\n",
        "!pip install -q -U tensorboard\n",
        "\n",
        "!pip install -q -U -i https://pypi.org/simple/ bitsandbytes\n",
        "\n",
        "# Ensure GPUtil is installed\n",
        "!pip install gputil"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.1.2 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.1.2 which is incompatible.\n",
            "torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires tensorboard<2.16,>=2.15, but you have tensorboard 2.16.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gputil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7394 sha256=40b8f043bdf79cedbce0a3d1eaa0a6d4f3f0a41bf6fb4ae07f3339526df925f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:50'"
      ],
      "metadata": {
        "id": "kvc9k_nADaJN"
      },
      "id": "kvc9k_nADaJN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import bitsandbytes as bnb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, PeftConfig\n",
        "from trl import SFTTrainer\n",
        "from trl import setup_chat_format\n",
        "from transformers import (AutoModelForCausalLM,\n",
        "                          AutoTokenizer,\n",
        "                          BitsAndBytesConfig,\n",
        "                          TrainingArguments,\n",
        "                          pipeline,\n",
        "                          logging)\n",
        "from sklearn.metrics import (accuracy_score,\n",
        "                             classification_report,\n",
        "                             confusion_matrix)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(f\"pytorch version {torch.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "onIuMeJmDgoI",
        "executionInfo": {
          "status": "error",
          "timestamp": 1715672713162,
          "user_tz": -330,
          "elapsed": 11,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "1a505a4d-2025-4e55-eaed-576ea7910108"
      },
      "id": "onIuMeJmDgoI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'bitsandbytes'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6ecf0dc14c49>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbitsandbytes\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bitsandbytes'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Disabling two features in PyTorch related to memory efficiency and speed during operations on the Graphics Processing Unit (GPU) specifically for the scaled dot product attention (SDPA) function."
      ],
      "metadata": {
        "id": "X-3iAjxaDxes"
      },
      "id": "X-3iAjxaDxes"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
        "torch.backends.cuda.enable_flash_sdp(False)"
      ],
      "metadata": {
        "id": "rWr0KpDoDkgq"
      },
      "id": "rWr0KpDoDkgq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "EuONe6pND5If"
      },
      "id": "EuONe6pND5If"
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/merged_raw_data.csv\")\n",
        "\n",
        "melted_df = pd.melt(df, id_vars=[\"Name\", \"Greenhouse \\nInterview\\nDecision\"],\n",
        "                    value_vars=[\"Part 2 - Turn 1\", \"Part 2 - Turn 2\", \"Part 2 - Turn 3\", \"Part 2 - Turn 4\"],\n",
        "                    var_name=\"Turn\", value_name=\"Response\")\n",
        "df = melted_df.rename(columns={'Greenhouse \\nInterview\\nDecision': 'Decision'})\n",
        "\n",
        "# Define a function to split the conversation into user and AI parts\n",
        "def split_conversation(text):\n",
        "    parts = text.split(\" AI: \")\n",
        "    user_part = parts[0].replace(\"User: \", \"\")  # Remove the 'User:' prefix\n",
        "    ai_part = parts[1] if len(parts) > 1 else ''  # Handle cases where there might not be an AI response\n",
        "    return pd.Series([user_part, ai_part])\n",
        "\n",
        "# Apply the function to the conversation column\n",
        "df[['Question', 'Answer']] = df['Response'].apply(split_conversation)\n",
        "\n",
        "df = df[df['Answer'].notna() & (df['Answer'] != '')]\n",
        "\n",
        "mapping = {'Yes': 'selected','Strong Yes': 'selected', 'Strong No': 'rejected', 'No': 'rejected'}\n",
        "df['Decision'] = df['Decision'].replace(mapping)\n",
        "\n",
        "print(df.value_counts('Decision'))\n",
        "\n",
        "X_train = list()\n",
        "X_test = list()\n",
        "for sentiment in [\"selected\", \"rejected\"]:\n",
        "    train, test  = train_test_split(df[df['Decision']==sentiment],\n",
        "                                    train_size=600,\n",
        "                                    test_size=200,\n",
        "                                    random_state=42)\n",
        "    X_train.append(train)\n",
        "    X_test.append(test)\n",
        "\n",
        "\"\"\"train, test  = train_test_split(df, test_size=0.2, random_state=42)\n",
        "X_train.append(train)\n",
        "X_test.append(test)\"\"\"\n",
        "\n",
        "X_train = pd.concat(X_train).sample(frac=1, random_state=10)\n",
        "X_test = pd.concat(X_test)\n",
        "\n",
        "eval_idx = [idx for idx in df.index if idx not in list(X_train.index) + list(X_test.index)]\n",
        "X_eval = df[df.index.isin(eval_idx)]\n",
        "X_eval = (X_eval\n",
        "          .groupby('Decision', group_keys=False)\n",
        "          .apply(lambda x: x.sample(n=50, random_state=10, replace=True)))\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "\n",
        "def generate_prompt(data_point):\n",
        "    return f\"\"\"\n",
        "      Classify an applicant's response to an interview question as \"selected\" or \"rejected\" for the purpose of hiring decisions. A \"selected\" classification suggests the candidate is suitable and should advance to the next round; a \"rejected\" classification suggests the candidate is not suitable for the role.\n",
        "\n",
        "      [QUESTION: {data_point[\"Question\"]} ANSWER: {data_point[\"Answer\"]}] = {data_point[\"Decision\"]}\n",
        "      \"\"\".strip()\n",
        "\n",
        "def generate_test_prompt(data_point):\n",
        "    return f\"\"\"\n",
        "      Classify an applicant's response to an interview question as \"selected\" or \"rejected\" for the purpose of hiring decisions. A \"selected\" classification suggests the candidate is suitable and should advance to the next round; a \"rejected\" classification suggests the candidate is not suitable for the role.\n",
        "\n",
        "      [QUESTION: {data_point[\"Question\"]} ANSWER: {data_point[\"Answer\"]}]  = \"\"\".strip()\n",
        "\n",
        "X_train = pd.DataFrame(X_train.apply(generate_prompt, axis=1),\n",
        "                       columns=[\"Response\"])\n",
        "X_eval = pd.DataFrame(X_eval.apply(generate_prompt, axis=1),\n",
        "                      columns=[\"Response\"])\n",
        "\n",
        "y_true = X_test.Decision\n",
        "X_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"Response\"])\n",
        "\n",
        "train_data = Dataset.from_pandas(X_train)\n",
        "eval_data = Dataset.from_pandas(X_eval)\n"
      ],
      "metadata": {
        "id": "1JIuzG7-D7hK"
      },
      "id": "1JIuzG7-D7hK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#useful functions"
      ],
      "metadata": {
        "id": "vnCIXp8JF_JM"
      },
      "id": "vnCIXp8JF_JM"
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(y_true, y_pred):\n",
        "    labels = ['selected', 'rejected']\n",
        "    mapping = {'selected': 1,'rejected': 0}\n",
        "    def map_func(x):\n",
        "        return mapping.get(x, 1)\n",
        "\n",
        "    y_true = np.vectorize(map_func)(y_true)\n",
        "    y_pred = np.vectorize(map_func)(y_pred)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
        "    print(f'Accuracy: {accuracy:.3f}')\n",
        "\n",
        "    # Generate accuracy report\n",
        "    unique_labels = set(y_true)  # Get unique labels\n",
        "\n",
        "    for label in unique_labels:\n",
        "        label_indices = [i for i in range(len(y_true))\n",
        "                         if y_true[i] == label]\n",
        "        label_y_true = [y_true[i] for i in label_indices]\n",
        "        label_y_pred = [y_pred[i] for i in label_indices]\n",
        "        accuracy = accuracy_score(label_y_true, label_y_pred)\n",
        "        print(f'Accuracy for label {label}: {accuracy:.3f}')\n",
        "\n",
        "    # Generate classification report\n",
        "    class_report = classification_report(y_true=y_true, y_pred=y_pred)\n",
        "    print('\\nClassification Report:')\n",
        "    print(class_report)\n",
        "\n",
        "    # Generate confusion matrix\n",
        "    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=[0, 1])\n",
        "    print('\\nConfusion Matrix:')\n",
        "    print(conf_matrix)"
      ],
      "metadata": {
        "id": "rUcUyAEPGEeC"
      },
      "id": "rUcUyAEPGEeC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(test, model, tokenizer):\n",
        "    y_pred = []\n",
        "    for i in tqdm(range(len(X_test))):\n",
        "        prompt = X_test.iloc[i][\"Response\"]\n",
        "        pipe = pipeline(task=\"text-generation\",\n",
        "                        model=model,\n",
        "                        tokenizer=tokenizer,\n",
        "                        max_new_tokens = 1,\n",
        "                        temperature = 0.0,\n",
        "                       )\n",
        "        result = pipe(prompt)\n",
        "        #print(result)\n",
        "        answer = result[0]['generated_text'].split(\"=\")[-1].strip()\n",
        "        print(answer)\n",
        "\n",
        "        if \"rejected\" in answer or \"RE\" in answer:\n",
        "            print(\"rejected\")\n",
        "            y_pred.append(\"rejected\")\n",
        "        elif \"selected\" in answer or \"SELECT\" in answer:\n",
        "            y_pred.append(\"selected\")\n",
        "            print(\"selected\")\n",
        "        else:\n",
        "            print(\"selected-l\")\n",
        "            y_pred.append(\"selected\")\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "6wfZU3HpGJJM"
      },
      "id": "6wfZU3HpGJJM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import GPUtil\n",
        "from datetime import datetime\n",
        "\n",
        "def gpu_usage():\n",
        "    gpus = GPUtil.getGPUs()\n",
        "    for gpu in gpus:\n",
        "        print(f\"GPU: {gpu.name}, Memory Used: {gpu.memoryUsed}MB, Total Memory: {gpu.memoryTotal}MB, Memory Utilization: {gpu.memoryUtil*100}%\")\n",
        "\n",
        "def monitor_gpu(fn):\n",
        "    def wrapper(*args, **kwargs):\n",
        "        print(\"GPU usage before running the function:\")\n",
        "        gpu_usage()\n",
        "        result = fn(*args, **kwargs)\n",
        "        print(\"GPU usage after running the function:\")\n",
        "        gpu_usage()\n",
        "        return result\n",
        "    return wrapper"
      ],
      "metadata": {
        "id": "zP1IeZyOHZRg"
      },
      "id": "zP1IeZyOHZRg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading model\n",
        "\n",
        "Next we need to take care of the model, which is a 8b-chat-hf (8 billion parameters, no RLHF, in the HuggingFace compatible format), loading from Kaggle models and quantization.\n",
        "\n",
        "Model loading and quantization:\n",
        "\n",
        "* First the code loads the Llama-3 language model from the Kaggle Models.\n",
        "* Then the code gets the float16 data type from the torch library. This is the data type that will be used for the computations.\n",
        "* Next, it creates a BitsAndBytesConfig object with the following settings:\n",
        "    1. load_in_4bit: Load the model weights in 4-bit format.\n",
        "    2. bnb_4bit_quant_type: Use the \"nf4\" quantization type. 4-bit NormalFloat (NF4), is a new data type that is information theoretically optimal for normally distributed weights.\n",
        "    3. bnb_4bit_compute_dtype: Use the float16 data type for computations.\n",
        "    4. bnb_4bit_use_double_quant: Do not use double quantization (reduces the average memory footprint by quantizing also the quantization constants and saves an additional 0.4 bits per parameter.).\n",
        "* Then the code creates a AutoModelForCausalLM object from the pre-trained Llama-2 language model, using the BitsAndBytesConfig object for quantization.\n",
        "* After that, the code disables caching for the model.\n",
        "* Finally the code sets the pre-training token probability to 1.\n",
        "\n",
        "Tokenizer loading:\n",
        "\n",
        "* First, the code loads the tokenizer for the Llama-3 language model.\n",
        "* Then it sets the padding token to be the end-of-sequence (EOS) token.\n",
        "* Finally, the code sets the padding side to be \"right\", which means that the input sequences will be padded on the right side. This is crucial for correct padding direction (this is the way with Llama 2)."
      ],
      "metadata": {
        "id": "qqK0-9gvGOyL"
      },
      "id": "qqK0-9gvGOyL"
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil -m cp -r gs://llm-finetune/peft/base_model/llama3/llama3-70b-chat-hf /content/"
      ],
      "metadata": {
        "id": "BrFFdPKgGXS4"
      },
      "id": "BrFFdPKgGXS4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"/content/llama3-70b-chat-hf\"\n",
        "\n",
        "compute_dtype = getattr(torch, \"float16\")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=compute_dtype,\n",
        "    quantization_config=bnb_config,\n",
        "    cache_dir='',\n",
        "    use_cache = False,\n",
        "    device_map = \"auto\",\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "max_seq_length = 2048\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, max_seq_length=max_seq_length)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id"
      ],
      "metadata": {
        "id": "2pHhNLg5GazK"
      },
      "id": "2pHhNLg5GazK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = predict(test, model, tokenizer)\n",
        "\n",
        "evaluate(y_true, y_pred)"
      ],
      "metadata": {
        "id": "Ds6yE-PDGf9u"
      },
      "id": "Ds6yE-PDGf9u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tunning\n",
        "\n",
        "In the next cell we set everything ready for the fine-tuning. We configures and initializes a Simple Fine-tuning Trainer (SFTTrainer) for training a large language model using the Parameter-Efficient Fine-Tuning (PEFT) method, which should save time as it operates on a reduced number of parameters compared to the model's overall size. The PEFT method focuses on refining a limited set of (additional) model parameters, while keeping the majority of the pre-trained LLM parameters fixed. This significantly reduces both computational and storage expenses. Additionally, this strategy addresses the challenge of catastrophic forgetting, which often occurs during the complete fine-tuning of LLMs.\n",
        "\n",
        "PEFTConfig:\n",
        "\n",
        "The peft_config object specifies the parameters for PEFT. The following are some of the most important parameters:\n",
        "\n",
        "* lora_alpha: The learning rate for the LoRA update matrices.\n",
        "* lora_dropout: The dropout probability for the LoRA update matrices.\n",
        "* r: The rank of the LoRA update matrices.\n",
        "* bias: The type of bias to use. The possible values are none, additive, and learned.\n",
        "* task_type: The type of task that the model is being trained for. The possible values are CAUSAL_LM and MASKED_LM.\n",
        "\n",
        "TrainingArguments:\n",
        "\n",
        "The training_arguments object specifies the parameters for training the model. The following are some of the most important parameters:\n",
        "\n",
        "* output_dir: The directory where the training logs and checkpoints will be saved.\n",
        "* num_train_epochs: The number of epochs to train the model for.\n",
        "* per_device_train_batch_size: The number of samples in each batch on each device.\n",
        "* gradient_accumulation_steps: The number of batches to accumulate gradients before updating the model parameters.\n",
        "* optim: The optimizer to use for training the model.\n",
        "* save_steps: The number of steps after which to save a checkpoint.\n",
        "* logging_steps: The number of steps after which to log the training metrics.\n",
        "* learning_rate: The learning rate for the optimizer.\n",
        "* weight_decay: The weight decay parameter for the optimizer.\n",
        "* fp16: Whether to use 16-bit floating-point precision.\n",
        "* bf16: Whether to use BFloat16 precision.\n",
        "* max_grad_norm: The maximum gradient norm.\n",
        "* max_steps: The maximum number of steps to train the model for.\n",
        "* warmup_ratio: The proportion of the training steps to use for warming up the learning rate.\n",
        "* group_by_length: Whether to group the training samples by length.\n",
        "* lr_scheduler_type: The type of learning rate scheduler to use.\n",
        "* report_to: The tools to report the training metrics to.\n",
        "* evaluation_strategy: The strategy for evaluating the model during training.\n",
        "\n",
        "SFTTrainer:\n",
        "\n",
        "The SFTTrainer is a custom trainer class from the TRL library. It is used to train large language models (also using the PEFT method).\n",
        "\n",
        "The SFTTrainer object is initialized with the following arguments:\n",
        "\n",
        "* model: The model to be trained.\n",
        "* train_dataset: The training dataset.\n",
        "* eval_dataset: The evaluation dataset.\n",
        "* peft_config: The PEFT configuration.\n",
        "* dataset_text_field: The name of the text field in the dataset.\n",
        "* tokenizer: The tokenizer to use.\n",
        "* args: The training arguments.\n",
        "* packing: Whether to pack the training samples.\n",
        "* max_seq_length: The maximum sequence length.\n",
        "\n",
        "Once the SFTTrainer object is initialized, it can be used to train the model by calling the train() method"
      ],
      "metadata": {
        "id": "tXXo78NoGpbH"
      },
      "id": "tXXo78NoGpbH"
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir=\"trained_weigths\"\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    r=64,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        ")\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,                    # directory to save and repository id\n",
        "    num_train_epochs=2,                       # number of training epochs\n",
        "    per_device_train_batch_size=1,            # batch size per device during training\n",
        "    gradient_accumulation_steps=8,            # number of steps before performing a backward/update pass\n",
        "    gradient_checkpointing=False,             # use gradient checkpointing to save memory\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    save_steps=0,\n",
        "    logging_steps=25,                         # log every 10 steps\n",
        "    learning_rate=2e-4,                       # learning rate, based on QLoRA paper\n",
        "    weight_decay=0.001,\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    max_grad_norm=0.3,                        # max gradient norm based on QLoRA paper\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.03,                        # warmup ratio based on QLoRA paper\n",
        "    group_by_length=False,\n",
        "    lr_scheduler_type=\"cosine\",               # use cosine learning rate scheduler\n",
        "    report_to=\"tensorboard\",                  # report metrics to tensorboard\n",
        "    evaluation_strategy=\"epoch\"               # save checkpoint every epoch\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_arguments,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=eval_data,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"Response\",\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=max_seq_length,\n",
        "    packing=False,\n",
        "    dataset_kwargs={\n",
        "        \"add_special_tokens\": False,\n",
        "        \"append_concat_token\": False,\n",
        "    }\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "BlFzd9lCG0F7"
      },
      "id": "BlFzd9lCG0F7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save trained model and tokenizer\n",
        "trainer.save_model()\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "metadata": {
        "id": "sPJljZAMG-HU"
      },
      "id": "sPJljZAMG-HU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs/runs"
      ],
      "metadata": {
        "id": "9638593AG_qe"
      },
      "id": "9638593AG_qe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = predict(test, model, tokenizer)\n",
        "evaluate(y_true, y_pred)"
      ],
      "metadata": {
        "id": "bHK_I6_lHBkJ"
      },
      "id": "bHK_I6_lHBkJ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "Fine-tunning Mixtral 8x7B"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}